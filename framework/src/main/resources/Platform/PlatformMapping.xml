<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<mappings>
    <pair>
        <name>spark</name>
        <platform>
            <docker-image>spark-env:v0</docker-image>

            <execution>
                <environment>/bin/sh -c</environment> <!--  -c: 从string中读取命令  -->
                <executor>/spark-runner/bin/spark-submit</executor>
                <args>
                    <!-- 平台级运行时参数 由用户或系统填入 -->
                    <!-- 按序读取、写入 -->
                    <arg name="--master" required="true" with-name="true">spark://spark-master-svc:7077</arg>
                    <arg name="--deploy-mode" required="true" with-name="true">cluster</arg>
                    <arg name="--name" required="true" with-name="true">spark-running</arg>
                    <arg name="--class" required="true" with-name="true">fdu.daslab.executable.spark.ExecuteSparkOperator</arg>
                    <arg name="--num-executors" required="true" with-name="true">2</arg>
                    <arg name="--driver-memory" required="true" with-name="true">32G</arg>
                    <arg name="--executor-memory" required="true" with-name="true">32G</arg>
<!--                    <arg name="&#45;&#45;conf=spark.default.parallelism" required="true" with-name="true">5</arg>-->
<!--                    <arg name="&#45;&#45;conf=spark.cores.max" required="true" with-name="true">3</arg>-->
                    <arg name="--conf=spark.executor.instance" required="true" with-name="true">2</arg>
                    <arg name="--conf=spark.kubernetes.container.image" required="true" with-name="true">spark:v0</arg>
                    <arg name="--conf=spark.kubernetes.namespace" required="true" with-name="true">argo</arg>
<!--                    <arg name="&#45;&#45;conf=spark.driver.host" required="true" with-name="true">$POD_IP</arg>-->
<!--                    <arg name="&#45;&#45;conf=spark.driver.bindAddress" required="true" with-name="true">0.0.0.0</arg>-->
<!--                    <arg name="&#45;&#45;conf=spark.driver.port" required="true" with-name="true">40091</arg>-->
<!--                    <arg name="&#45;&#45;conf=spark.kubernetes.driver.pod.name" required="true" with-name="true">$POD_NAME</arg>-->
<!--                    <arg name="&#45;&#45;conf=spark.kubernetes.authenticate.driver.serviceAccountName" required="true" with-name="true">spark</arg>-->
                    <arg name="package" required="true" with-name="false">/data/jars/executable-spark.jar</arg>
                    <arg name="--udfPath" required="true" with-name="true"> </arg>
                </args>
            </execution>

            <!-- 平台自定义属性，framework仅负责传递，不进去看都有什么 -->
            <properties>
                <property name="master">local[2]</property>
            </properties>
        </platform>
    </pair>

    <pair>
        <name>java</name>
        <platform>
            <docker-image>java-env:v0</docker-image>

            <execution>
                <environment>/bin/sh -c</environment>
                <executor>java -jar</executor>
                <args>
                    <!-- 平台级运行时参数 -->
                    <arg name="package" required="true" with-name="false">/data/jars/executable-java.jar</arg>
                    <arg name="--udfPath" required="true" with-name="true"> </arg>
                </args>
            </execution>

        </platform>
    </pair>

    <pair>
        <name>pytorch</name>
        <platform>
            <docker-image>pytorch:v1</docker-image>

            <execution>
                <environment>/bin/sh -c</environment>
                <executor>python</executor>
                <args>
                    <!-- 平台级运行时参数 -->
                    <arg name="package" required="true" with-name="false">/data/jars/clic-pytorch/Executor.py</arg>
                </args>
            </execution>

        </platform>
    </pair>

    <pair>
        <name>spark-ml</name>
        <platform>
            <docker-image>spark-ml:v1</docker-image>

            <execution>
                <environment>/bin/sh -c</environment>
                <executor>python</executor>
                <args>
                    <!-- 平台级运行时参数 -->
                    <arg name="package" required="true" with-name="false">/data/jars/clic-pytorch/Executor.py</arg> <!-- 这里还没改成sparkML执行文件的目录  -->
                </args>
            </execution>

        </platform>
    </pair>

    <pair>
        <name>graphchi</name>
        <platform>
            <docker-image>java-env:v0</docker-image>

            <execution>
                <environment>/bin/sh -c</environment>
                <executor>java -jar</executor>
                <args>
                    <!-- 平台级运行时参数 -->
                    <arg name="package" required="true" with-name="false">/data/jars/executable-graphchi.jar</arg>
                    <arg name="--udfPath" required="true" with-name="true"> </arg>
                </args>
            </execution>
        </platform>
    </pair>

    <pair>
        <!-- 可以是随便一个C++的平台，以CMake构建的MPI工程为例 -->
        <name>alchemist</name>
        <platform>
            <docker-image>executable/alchemist:v1</docker-image>

            <execution>
                <environment>/bin/sh -c</environment>
                <executor>mpiexec</executor> <!-- mpi在MacOS & Linux上的executor -->
                <args>
                    <arg name="-n" with-name="true">1</arg> <!-- 没办法，1能避免socket错误 -->
                    <arg name="package" with-name="false" required="true">alchemist</arg> <!-- 入口程序 可执行文件 -->
                    <arg name="--udfPath" required="true" with-name="true"> </arg> <!--  xx.dylib、 xx.dll -->
                </args>
            </execution>

            <properties>
                <property name="OS">linux</property>
            </properties>
        </platform>
    </pair>

</mappings>